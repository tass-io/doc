# Process-level Schedule Draft

Process level schedule 主要分为四个方面：

1. 每个 process 的资源配置；
2. Process 的 keep alive 策略；
3. Process 水平扩展策略；
4. 预启动策略。

## Resource Provisioning

### Motivation

在常规 FaaS 中，采用容器级别的抽象区分资源和 namespace，在 Tass 中，我们做了 Workflow 和 Function 两层的逻辑抽象，很自然的问题是，该如何对这些抽象提供资源配置。

### Design

对于 Workflow 而言，它们只存在逻辑上的抽象而不存在物理资源的抽象。也就是说一台机器上可以有多个 WorkflowRuntime，它们共享一个物理资源池。

> ⚠️ 这里会存在一个多租户饥饿的问题，以及随之而来可能的 reschedule 问题，这些是 Workflow 层级的调度，放在后续讨论。

Tass 中 Local scheduler 在创建进程时会进行一定的资源限制，资源的限制全权交由用户定制，从而避免一些进程在 runtime 时占用更多资源，这种资源的分配也可以简化后续其他策略的设定。静态分配资源的方式会存在一个问题，那就是有可能出现资源过度分配或者资源分配不足。

理论上来说，可以使用一种动态资源分配的方式给每个进程，从实现的角度来说，可以通过 cgroupv2 来实现，但是动态分配的事情极大增加了实现的复杂度，

目前，Tass 打算把**资源分配问题丢给用户决定**，即采用<u>静态资源配置</u>。

## Keep-alive Policy

### Motivation

当一个函数请求发往 Tass 的时候，有可能出现三种情况：

1. 未存在一个相关函数实例，启动一个该函数的进程；
2. 进程处于 warm/running 状态，请求直接发往该进程；
3. 该函数进程存在，但是无法接受更多请求，创建一个新的进程处理请求。

对于第一种和第三种情况，请求的执行时间如下：

```tex
|<---Explicit Init---><---Function Execution---> 
```

和其他的 FaaS 一样，在 Tass 中，我们称 Explicit Init 时间为冷启动时间，只不过这里的冷启动是进程级别的冷启动。显然，如果一个进程处于 warm 状态，它就可以直接执行请求，本质上我们肯定希望每个函数都处于 warm 状态，从而规避冷启动。然而，从资源利用的角度来说，资源是存在上限的，因此，进程执行完请求后是“保活”一段时间，这里的基本思想在于一个最近被调用的函数有很大的概率在不久的将来再次被调用。但是这种思想存在一个关键问题，“保活”多久？在所有其他的 FaaS 平台中，“保活”一直是一个固定值，比如 OpenWhisk 的 TTL 是固定 10 分钟。有两点驱动我们在 Tass 中重新思考 keep alive 的策略设计：

1. Tass 中函数的执行单位，它创建开销相比容器要轻很多，我们有理由给这里面的进程设置更短的 TTL；
2. 为了减少函数间通信的开销，Tass 将同一个 Workflow 下的不同函数**尽可能放置在同一台机器中**，然而一台机子上的资源是有限的，随着进程的创建，过长的保活时间将导致本地资源的不足而无法创建新的进程，从而使得新的进程创建到其他节点上，增加了通信的开销，因此，一种有效的驱逐策略是有意义的。

### Design

这里提出一种混合式的 keep alive 策略，分为两部分：

第一部分为每个进程的 TTL 上限，和现有的 FaaS 系统类似，每个进程的都有一个固定的 TTL（1分钟），每当一个进程被调用的时候，它的 TTL 就会重置会 1 分钟，当一个进程在到达 TTL 仍未收到新的请求时，它就会被终止。

由于每一台机器的资源存在上限，因此还需要有一种驱逐策略能在资源到达上限时终止合适的进程，受 [FaaSCache](https://dl.acm.org/doi/10.1145/3445814.3446757) 论文的启发，使用 GDSF 算法的变种，对于每一个正在运行的进程，它都有一个优先级，优先级的计算公式如下：
$$
Priority = Clock + \frac{Freq \times Cost}{Size}
$$
当一个 Warm 函数被重用了，它的优先级就会被更新，接下来将重点解释这几个参数的含义：

* **Clock**：Clock 用来记录函数的实效性，每个机器会维护一个逻辑时钟，在每次发生驱逐时更新。当一个进程被使用时，对应的 Clock 就会被更新，那么它的优先级就会变大，因此，最近没有使用的进程就会有较小的 Clock。当没有足够资源启动新进程，而现有的 warm 进程无法被使用时，一个进程就会被终止。具体来说，如果一个进程 j 被终止了，那么 j 一定是最小优先级，那么 $Clock = Priority_j$，此后被使用的进程在更新自身 Clock 值时就会使用改值；
* **Frequency**：Frequency 是指一个给定函数被调用的次数。一个函数可以被多个进程执行，Frequency 表示其所有容器中函数调用的总次数。当一个函数的所有进程被终止时，Frequency 被设置为零。优先级与频率成正比，因此，更频繁执行的函数会保持更长的时间。在 Tass Workflow 的场景下，Freq 和这个函数在 Workflow 中被调用的概率息息相关，在一个 Workflow 中，每个函数被调用的概率是不同的，对于 workflow 的关键路径上的函数，它们的调用概率显然为 100%，但是对于在一些选择分支上的函数，被调用的概率就会低很多，这意味着它们的 Freq 也会低很多；
* **Cost**：Cost 代表终止成本，它等于总的初始化时间，这体现了保持进程存活的好处以及冷启动的成本。因此，优先级和初始化开销成正比；
* **Size**：Size 是进程的 resource footprint，在大多数情况下，可以运行的进程数量受物理内存可用性的限制，因为 CPU 很容易被复用但是内存交换会导致严重的性能下降，因此这里的 Size 考虑的是容器内存的使用。

因此，一个进程有两种情况会被终止，一种是到达 TTL 终止，一种是被驱逐策略杀死。两种策略分别对应不同的目的，如果只是从单机上的一个 workflow 资源的角度来说，那么 TTL 的设置并非必要，它的所有函数都可以保持 warm 的状态，直到必须被驱逐的那一刻。但是实际上一台机器上可以运行多个 workflow，由于不同 workflow 存在逻辑隔离，不同 workflow 的 function 是互相透明的。在这种情况下，从物理资源的角度来说确实也可以采取驱逐方式驱逐整台机子下优先级低的函数，但是这就会使同一台机器上的不同租户受到对方的影响。TTL 的存在可以保证一个进程的存活上限，使之在空闲一段时间后会被终止而使得其他 workflow 的进程有机会部署。

如果 local scheduler 要创建一个进程的时候发现本地没有足够资源创建，那么 local schedule 会执行以下行为：

1. 先尝试 Terminating 本地优先级低的进程，后续请求被阻塞，等待被终止的进程彻底结束服务；
2. 在等待进程结束时，如果到达一定 timeout 后该进程资源仍然未被释放，local scheduler 将：
   1. 检查其他节点的相同 workflow 是否存在下游的进程，存在则将数据发送至对应的节点；
   2. 否则尝试发送数据到其他资源可用的 workflow 上，由它负责后续进程的创建和回收；
   3. 上述这种尝试存在一定的跳数限制，即若连续 2 跳 Workflow 创建新进程都处于失败，说明此时集群处于资源极度紧张的情况，将不再继续尝试创建进程，返回给用户资源不足的报错。

为了保证 locality，Local scheduler 总是尝试优先在本地尝试进程的创建，如果无法创建再走 HTTP。

## Auto-scaler Policy

### Motivation

FaaS 平台中使用容器级别的自动扩缩容来应对高峰请求，在 Tass 中，也需要处理自动扩缩容。自动扩缩容源自一个非常自然的想法，单个进程能够处理的请求存在上限，当后续请求再次来临时，要么请求直接丢弃，要么处理延迟大幅度增加。

在 Tass 中，尽管每次调用需要流经整个 Workflow，但是每个函数的处理能力其实是不同的，这意味着有些进程可能每秒可以处理 1000 个请求，而有些进程每秒只能处理几十个请求；或者是有些进程需要处理全部发来的请求，因为处在关键路径上，而有些进程只需要处理其中的一部分。这表明，Workflow 中的进程的扩缩容可以独立处理。

### Design

从设计的角度来说，扩缩容主要包含三种策略：

* 基于消息扩容，即根据请求数量的多寡决定扩缩容；
* 基于 footprint 的在线扩容，即根据当前进程的资源占用扩容，如当内存使用到达 80% 的阈值时扩容；
* 混合式策略扩容：
  * 消息扩容 + 基于 footprint 的在线扩容，两者谁先满足条件就出发扩容；
  * 基于内存的在线扩容 + 基于历史数据的离线扩容：后者主要是根据历史数据提前预测即将有较大数量的请求到达，触发扩容。

Tass 中目前采用第二种。

## Pre-start Policy

### Motivation

Pre-start 来源于一个简单的驱动力，链式函数调用的级联开销。我们已经知道链式函数的第一次调用每个函数都会面临冷启动问题，那么当一个链式调用触发的时候，函数第一次的冷启动就会带来级联开销。

一个链式调用的链路越长，冷启动带来的额外开销时间就越长。目前，所有 FaaS 平台并没有对链式调用的函数进行任何优化。

如果我们能在 workflow 运行一段时间获取 workflow 整体运行情况，了解每个进程平均执行时间，以及在分支选择上每个进程的执行概率，我们就可以利用这个模型预启动进程，减少级联开销。

### Design

在 Tass 没有拥有 workflow 的先验知识时，启动 workflow 将退化到最差情况：即每一个函数运行结束后，根据函数结果和控制逻辑决定下一个即将运行的进程，local scheduler 启动该进程，等进程启动完毕后，再将请求发往该进程执行用户代码逻辑。

然而，当我们的函数执行一段时间后，Tass 就会得到这个 Workflow 的整体运行情况的一个模型，从而可以根据模型情况预先启动函数。

> TODO：补两个图解释一下